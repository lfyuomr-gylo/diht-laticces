{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение 1\n",
    "\n",
    "Для классификации объекта посчитаем, сколько в положительном контексте объектов, мощность пересечения признаков которых с признаками данного объекта составляет долю, большую чем `C` от общего числа признаков классифицируемого объекта. Каждый такой объект в положительном контексте будем называть \"голосом\" за положительную классификацию объекта.\n",
    "\n",
    "Аналогичным образом посчитаем голоса за отрицательную классификацию. Каких голосов будет больше, те и будем использовать.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier1(positive_context, negative_context, object_props, c=0.5):\n",
    "    def count_votes(context):\n",
    "        votes = 0\n",
    "        for props in context:\n",
    "            intersection_size = object_props & props\n",
    "            if len(intersection_size) >= c * len(object_props):\n",
    "                votes += 1\n",
    "        return votes\n",
    "\n",
    "    positive_votes = count_votes(positive_context)\n",
    "    negative_votes = count_votes(negative_context)\n",
    "    return 1 if positive_votes > negative_votes else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение 2\n",
    "\n",
    "Чтобы классифицировать объект, будем для каждого контекста считать количество объектов, пересечение свойств которых со свойствами классифицируемого объекта не встречается в противоположном контексте.\n",
    "\n",
    "В каком контексте насчитали больше таких объектов, к такому классу и относим объект"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier2(positive_context, negative_context, object_props):\n",
    "    def count_votes(target_context, opposite_context):\n",
    "        votes = 0\n",
    "        for props in target_context:\n",
    "            props_intersection = object_props & props\n",
    "            if not any(props_intersection <= opposite_props for opposite_props in opposite_context):\n",
    "                votes += 1\n",
    "        return votes\n",
    "\n",
    "    positive_votes = count_votes(positive_context, negative_context)\n",
    "    negative_votes = count_votes(negative_context, positive_context)\n",
    "    return 1 if positive_votes > negative_votes else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование эффективности и оптимизация первого решения\n",
    "\n",
    "Для начала, сделаем некоторые подготовительные действия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    \"\"\"    (file_name) -> (positive_context, negative_context)    \"\"\"\n",
    "    positive_context = list()\n",
    "    negative_context = list()\n",
    "    with open(file_name, \"r\") as f:\n",
    "        # skip the header\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            columns = [x.strip() for x in line.split(',')]\n",
    "            props = set(i for (i, x) in enumerate(columns[:-1]) if x == 'x')\n",
    "            if columns[-1] == 'positive':\n",
    "                positive_context.append(props)\n",
    "            else:\n",
    "                negative_context.append(props)\n",
    "    return {\"positive\": positive_context, \"negative\": negative_context}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_scores(train, test, classifier):\n",
    "    y_correct = [1] * len(test['positive']) + [-1] * len(test['negative'])\n",
    "    y_predicted = []\n",
    "    objects = test['positive'] + test['negative']\n",
    "    for obj_props in objects:\n",
    "        y_predicted.append(classifier(train['positive'], train['negative'], obj_props))\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_correct, y_predicted),\n",
    "        'precision': precision_score(y_correct, y_predicted),\n",
    "        'recall': recall_score(y_correct, y_predicted)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим на метрики, которые получаются при разных значениях параметра `C` в первом решении. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\tAccuracy\tPrecision\tRecall\n",
      "0.5\t0.6534736343466454\t0.6534736343466454\t1.0\n",
      "0.55\t0.6720720754101499\t0.6659386417820471\t1.0\n",
      "0.6\t0.6720720754101499\t0.6659386417820471\t1.0\n",
      "0.65\t0.6794298894178069\t0.6708925608078662\t1.0\n",
      "0.7\t0.6994726500753474\t0.6850787653048859\t1.0\n",
      "0.75\t0.6994726500753474\t0.6850787653048859\t1.0\n",
      "0.8\t0.8372087446601346\t0.8021675858255325\t1.0\n",
      "0.85\t0.7817489504433107\t0.7952091194706027\t0.9014572374872412\n",
      "0.9\t0.7817489504433107\t0.7952091194706027\t0.9014572374872412\n",
      "0.95\t0.7817489504433107\t0.7952091194706027\t0.9014572374872412\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "trains = [load_data(f'train/train{i}.csv') for i in range(1, 11)]\n",
    "tests = [load_data(f'test/test{i}.csv') for i in range(1, 11)]\n",
    "\n",
    "print(\"C\\tAccuracy\\tPrecision\\tRecall\")\n",
    "for c in range(50, 100, 5):\n",
    "    c /= 100\n",
    "    scores = []\n",
    "    for train, test in zip(trains, tests):\n",
    "        scores.append(measure_scores(train, test, partial(classifier1, c=c)))\n",
    "    accuracy = np.mean(list(score['accuracy'] for score in scores))\n",
    "    precision = np.mean(list(score['precision'] for score in scores))\n",
    "    recall = np.mean(list(score['recall'] for score in scores))\n",
    "    print(f\"{c}\\t{accuracy}\\t{precision}\\t{recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, данное решение лучше всего себя ведет при `c = 0.8`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Второе же решение, как оказывается, вообще полностью решает нашу задачу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0}\n",
      "{'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0}\n",
      "{'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0}\n",
      "{'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0}\n",
      "{'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0}\n",
      "{'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0}\n",
      "{'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0}\n",
      "{'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0}\n",
      "{'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0}\n",
      "{'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0}\n"
     ]
    }
   ],
   "source": [
    "for train, test in zip(trains, tests):\n",
    "    print(measure_scores(train, test, classifier2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
